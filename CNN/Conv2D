import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Flatten, Dropout
from sklearn.model_selection import train_test_split

# 1. 범용적인 split_sequence 함수 정의 (다양한 시계열 데이터 적용 가능)
def split_sequence(sequence, n_steps):
    """
    시계열 데이터를 n_step 길이로 분할하는 함수.
    - sequence: 전체 데이터 시퀀스 (리스트나 배열 형태)
    - n_steps: 시퀀스를 나누는 기준이 되는 step 수
    
    출력: 입력 시퀀스 X, 출력 시퀀스 y
    """
    X, y = list(), list()
    for i in range(len(sequence)):
        end_ix = i + n_steps
        if end_ix > len(sequence)-1:
            break
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

# 2. 예시 데이터셋 생성 (랜덤 시계열 데이터)
np.random.seed(42)
data = np.random.rand(1000)  # 1000개의 랜덤 시계열 데이터 생성
n_steps = 3  # 시계열 데이터를 n=3 스텝씩 나눔

# 시계열 데이터를 split_sequence 함수로 나눔
X, y = split_sequence(data, n_steps)

# 3. CNN+LSTM 모델을 위한 입력 데이터의 형상 조정
# CNN의 2D 입력에 맞게 (샘플 수, 행 수, 열 수, 채널 수) 형상으로 변환해야 함
# 2D Conv를 위해 2차원 형태로 변환 (예: (샘플 수, 타임스텝 수, 1차원 입력의 너비, 채널 수))
X = X.reshape((X.shape[0], X.shape[1], 1, 1))  # (샘플 수, 시계열 스텝 수, 특징 수, 채널 수)

# 4. 학습 데이터와 테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. CNN + LSTM 모델 정의 (2D Conv 사용)
model = Sequential()

# (1) Conv2D 층: CNN을 사용하여 시계열 데이터의 지역적 패턴을 학습
# - Conv1D 대신 Conv2D를 사용하며, 2D로 확장된 시계열 데이터에서 패턴을 추출
# - filters: CNN 필터 개수, kernel_size: 필터 크기
model.add(Conv2D(filters=64, kernel_size=(2, 1), activation='relu', input_shape=(n_steps, 1, 1)))

# (2) MaxPooling2D 층: MaxPooling을 사용해 데이터 크기를 줄임
# - GlobalMaxPooling2D를 사용하면 전체 데이터에서 최대값 추출 가능
model.add(MaxPooling2D(pool_size=(2, 1)))

# (3) Dropout 층: Dropout을 사용해 과적합을 방지 (0.2 = 20% 노드 비활성화)
# - Dropout 대신 BatchNormalization을 사용할 수 있음 (네트워크 안정화)
model.add(Dropout(0.2))  # Dropout을 통해 과적합 방지

# (4) Flatten 층: 2D 데이터를 1D로 펼쳐서 LSTM에 입력할 준비를 함
# - Flatten을 통해 Conv 층의 출력을 LSTM에 맞게 변환
model.add(Flatten())

# (5) LSTM 층: CNN에서 추출한 특징을 기반으로 시계열 정보를 처리
# - LSTM 대신에 GRU를 사용할 수 있음 (GRU는 LSTM보다 경량화된 RNN 구조)
model.add(LSTM(50, activation='relu', return_sequences=False))  # return_sequences=False로 마지막 출력만 사용

# (6) 완전 연결층(Dense Layer)
# - Dense 층은 CNN+RNN에서 추출한 특징을 종합해 예측을 수행
model.add(Dense(50, activation='relu'))

# (7) 출력층 (회귀 문제를 가정하므로 활성화 함수는 없거나 선형 사용)
model.add(Dense(1))  # 회귀 문제에서는 단일 값을 출력

# 6. 모델 컴파일: 회귀 문제이므로 MSE 손실 함수 사용
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# 7. 모델 학습
model.fit(X_train, y_train, epochs=50, verbose=1, validation_data=(X_test, y_test))

# 8. 평가
mse, mae = model.evaluate(X_test, y_test, verbose=1)
print(f"Test MSE: {mse}, Test MAE: {mae}")
